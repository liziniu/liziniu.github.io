<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
<title></title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="publication.html">Publication</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="https://imitation-learning-blog.github.io/">Blog</a></div>
</td>
<td id="layout-content">
<h2>Research</h2>
<p>&ldquo;To complicate is easy. To simplify is difficult.&rdquo; &ndash; Bruno Munari</p>
<p>My research focuses on machine learning applications and aims to design effective and efficient algorithms and provide the associated theoretical analysis. Existing research results (finished with many collaborators) are summarized below.</p>
<h3>Imitation Learning</h3>
<p>Background: Imitation learning trains good policies from expert/human demonstrations, with applications in autonomous driving, robotics, etc. <br />
Many algorithms, such as behavioral cloning (BC) and adversarial imitation learning (AIL), have been developed. <br />
However, the theoretical foundation remains under-developed: which algorithm performs better? in what scenario?</p>
<p>Contribution:</p>
<ul>
<li><p>A general framework to study the error bounds of imitating policies and environments.[1][2]</p>
</li>
</ul>
<ul>
<li><p>A reduction of off-line AIL methods to BC.[3]</p>
</li>
</ul>
<ul>
<li><p>A horizon-free sample complexity bound to explain and support the superior performance of online AIL methods.[4]</p>
</li>
</ul>
<ul>
<li><p>An efficient algorithm to address the interaction efficiency of online AIL methods.[5]</p>
</li>
</ul>
<ul>
<li><p>A theoretical analysis of imitation learning with supplementary dataset.[6]</p>
</li>
</ul>
<h3>Reinforcement Learning</h3>
<p>General Background: Reinforcement learning (RL) refers to a class of algorithms that solve long-term decision-making problems. <br />
My research covers many aspects of RL, which will be explained below.</p>
<h4>Exploration</h4>
<p>Background: Typically, RL applications have huge state-action space (i.e., enormous decision choices) and noisy feedback (due to transition and reward noise). <br />
A scenario is the online setting, where the agent interacts with the environment to improve performance based on the collected data. <br />
In this case, RL algorithms have to explore uncertain actions when maximizing the long-term return. <br />
The key research problem is to improve the exploration (i.e., data-collection) efficiency.</p>
<p>Contribution:</p>
<ul>
<li><p>A intrinsically motivated exploration method that performs well especially on SuperMarioBros games.[7]</p>
</li>
</ul>
<ul>
<li><p>A randomized exploration method that solves many challenging tasks with cheap computation.[8]</p>
</li>
</ul>
<h4>Training</h4>
<p>Background: In addition to the exploration issue, RL methods need to solve non-linear Bellman equations when training. <br />
The key problems include but are not limited to numerical stability and computation efficiency.</p>
<p>Contribution:</p>
<ul>
<li><p>A theoretical verification of target Q-learning, an algorithm that is widely used in practice, in the tabular setting.[9]</p>
</li>
</ul>
<ul>
<li><p>A heuristic-guided black-box optimization algorithm for deep RL (and other derivative-free problems).[10]</p>
</li>
</ul>
<h3>Reference</h3>
<p>[1] Xu, T., Li, Z., and Yu, Y. Error Bounds of Imitating Policies and Environments. NeurIPS 2020.</p>
<p>[2] Xu, T., Li, Z., and Yu, Y. Error Bounds of Imitating Policies and Environments for Reinforcement Learning. TPAMI 2021.</p>
<p>[3] Li, Z., Xu, T., Yu, Y., and Luo, Z.-Q. Rethinking ValueDice: Does It Really Improve Performance? ICLR 2021.</p>
<p>[4] Xu, T., Li, Z., Yu, Y., and Luo, Z.-Q. Understanding Adversarial Imitation Learning in Small Sample Regime: A Stage-coupled Analysis. arXiv:2208.01899.</p>
<p>[5] Xu, T., Li, Z., Yu, Y., and Luo, Z.-Q. Provably Efficient Adversarial Imitation Learning with Unknown Transitions. UAI 2023.</p>
<p>[6] Li, Z., Xu, T., Yu, Y., and Luo, Z.-Q. Theoretical Analysis of Offline Imitation With Supplementary Dataset. arXiv:2301.11687.</p>
<p>[7] Li, Z., and Chen, X.-H. Efficient Exploration by Novelty-Pursuit. DAI 2020.</p>
<p>[8] Li, Z., Li, Y., Zhang, Y., Zhang, T., and Luo, Z.-Q. HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning. ICLR 2022.</p>
<p>[9] Li, Z., Xu, T., and Yu, Y. A Note on Target Q-learning For Solving Finite MDPs with A Generative Oracle. arXiv:2203.11489.</p>
<p>[10] Liu, F.-Y., Li, Z., and Qian, C. Self-Guided Evolution Strategies with Historical Estimated Gradients. IJCAI 2020</p>
</td>
</tr>
</table>
</body>
</html>
