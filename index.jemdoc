# jemdoc: menu{MENU}{index.html}, nofooter
==Ziniu Li

~~~
{}{img_left}{images/liziniu3.jpg}{alt text}{210}{280}{}
Ph.D. student,\n
[https://sds.cuhk.edu.cn/en School of Data Science], \n
[https://www.cuhk.edu.cn/en The Chinese University of Hong Kong, Shenzhen]\n

Email: ziniuli@link.cuhk.edu.cn

[https://twitter.com/ZiniuLi \[Twitter\] ] [https://www.zhihu.com/people/mo-fei-10-41 \[Zhihu\] ]
~~~

== About me

I am a Ph.D. student at The Chinese University of Hong Kong, Shenzhen (CUHKSZ), advised by [https://scholar.google.com/citations?user=dW3gcXoAAAAJ&hl=en Prof. Zhi-Quan (Tom) Luo].

I am interested in artificial intelligence, especially reinforcement learning and large language models.

I have worked/interned at Tencent, Nanjing University, Cardinal Operations, etc.

Feel free to contact me if you want to discuss some ideas.

== Research Statement

My research focuses on the algorithm design and theoretical analysis of machine learning models, particularly in reinforcement learning. Currently, I am primarily working on large language models to pursue the development of an agent capable of intelligent multi-task performance. My previous research outcomes are highlighted below:

In the field of large language models, my work spans several key areas: data selection ([https://openreview.net/forum?id=vO04AzsB49 NeurIPS 2023, Spotlight]), diversity-preserving supervised fine-tuning ([https://arxiv.org/abs/2408.16673 ICLR 2025]), computationally efficient RLHF ([https://arxiv.org/abs/2310.10505 ICML 2024]), and hallucination mitigation ([https://openreview.net/pdf?id=kX2EqGUp5B ICLR 2025]).

In the field of imitation learning and reinforcement learning, I am interested in the theory of sample complexity ([https://arxiv.org/abs/2010.11876 NeurIPS 2020], [https://ieeexplore.ieee.org/document/9485061 TPAMI 2021], [https://arxiv.org/abs/2306.06563 UAI 2023, Oral]), as well as applications in robotics ([https://iclr-blog-track.github.io/2022/03/25/rethinking-valuedice ICLR 2024 Blog]) and signal processing ([https://ieeexplore.ieee.org/document/10634527 TSP 2024]).

I also work on optimization-centric topics with other researchers, including understanding Adam in training Transformers ([https://arxiv.org/abs/2402.16788 NeurIPS 2024]), memory-efficient optimizers ([https://arxiv.org/abs/2406.16793 ICLR 2025]), zero-order optimization ([https://www.ijcai.org/proceedings/2020/205 IJCAI 2020]), and prompt-tuning ([https://aclanthology.org/2024.findings-emnlp.871.pdf EMNLP 2024]). 

== Recent Highlights

*: indicating equal contribution or alphabetic ordering.


~~~
  [https://openreview.net/forum?id=NQEe7B7bSw Preserving Diversity in Supervised Fine-tuning of Large Language Models] \n
   *Ziniu Li*, Congliang Chen, Tian Xu, Zeyu Qin, Jiancong Xiao, Ruoyu Sun, Zhi-Quan Luo \n
  (The 13th International Conference on Learning Representations (ICLR), 2025) \n
  ({{<span style="color:red">Best Paper Runner-up</span>}} at NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability)
  
  TL;DR: /This work introduces a game-theoretic distribution matching method to address the diversity-reducing and knowledge-forgetting issues in SFT /
~~~

~~~
  [https://arxiv.org/abs/2310.10505 ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models] \n
  *Ziniu Li*, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, Zhi-Quan Luo \n
  (The 41st International Conference on Machine Learning (ICML), 2024) 

  TL;DR: /This work shows that PPO overshoots for RLHF in LLMs and introduces ReMax, which requires half the memory of PPO and runs twice as fast /
~~~

~~~
  [https://openreview.net/forum?id=lNEFatlsQb When is RL better than DPO in RLHF? A Representation and Optimization Perspective] \n
  *Ziniu Li\* *, Tian Xu\*, Yang Yu \n
  ({{<span style="color:red">Oral Presentation</span>}}, The 12th International Conference on Learning Representations (ICLR) (Tiny Paper Track), 2024)

  TL;DR: /This work analyzes the reward modeling quality in view of representations and the optimization error sources /
~~~


~~~
  [https://openreview.net/forum?id=vO04AzsB49 Imitation Learning from Imperfection: Theoretical Justifications and Algorithms] \n
  *Ziniu Li\* *, Tian Xu\*, Zeyu Qin, Yang Yu, Zhi-Quan Luo \n
  ({{<span style="color:red">Spotlight Presentation</span>}}, In Neural Information Processing System (NeurIPS) 37, 2023)

  TL;DR: /This work validates that importance sampling is effective in data selection when leveraging multiple imperfect (out-of-distribution and low-quality) data sources/
~~~



== Service

==== Reviewer

NeurIPS ([https://neurips.cc/Conferences/2022/ProgramCommittee Top Reviewer]), ICML ([https://icml.cc/Conferences/2022/Reviewers Outstanding Reviewer]), ICLR ([https://iclr.cc/Conferences/2022/Reviewers Highlighted Reviewer]).

==== Teaching Assistant

- DDA6111: Discrete Optimization. 2022 Spring @ CUHKSZ

- DDA6060: Machine Learning. 2023 Spring @ CUHKSZ

- FTE4560: Basic Machine Learning. 2021 Spring @ CUHKSZ.

- CSC4120: Design and Analysis of Algorithms. 2022 Fall, 2021 Fall @ CUHKSZ

- MAT3007: Introduction to Optimization. 2020 Fall @ CUHKSZ

==== Lecturer

- Machine Learning (Summer Course for Senior High School Students) @ X ACADEMY 2022 TechX

== Award

- \[2024-12\] Best Paper Runner-up at NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability

- \[2024-01\] Runner-up of poster presentation award at the third doctoral and postdoctoral forum of Shenzhen Research Institute of Big Data. $5,000 CNY

- \[2023-12\] Guo-Tai-Jun-An Scholarship. $20,000 CNY

- \[2021-04\] Best oral presentation award at the first doctoral and postdoctoral forum of Shenzhen Research Institute of Big Data. $5,000 CNY
