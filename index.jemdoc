# jemdoc: menu{MENU}{index.html}, nofooter
==Ziniu Li

~~~
{}{img_left}{images/liziniu3.jpg}{alt text}{210}{280}{}
Ph.D. student,\n
[https://sds.cuhk.edu.cn/en School of Data Science], \n
[https://www.cuhk.edu.cn/en The Chinese University of Hong Kong, Shenzhen]\n

Email: ziniuli@link.cuhk.edu.cn

[https://twitter.com/ZiniuLi \[Twitter\] ] [https://www.zhihu.com/people/mo-fei-10-41 \[Zhihu\] ]
~~~

== About me

I am a Ph.D. student at The Chinese University of Hong Kong, Shenzhen (CUHKSZ), advised by [https://scholar.google.com/citations?user=dW3gcXoAAAAJ&hl=en Prof. Zhi-Quan (Tom) Luo].

I am interested in artificial intelligence, especially reinforcement learning and large language models.

I have worked/interned at Tencent, Nanjing University, Cardinal Operations, etc.

Feel free to contact me if you want to discuss some ideas.

== Recent Highlights

*: indicating equal contribution or alphabetic ordering.


~~~
  [https://arxiv.org/abs/2408.16673 Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity] \n
   *Ziniu Li*, Congliang Chen, Tian Xu, Zeyu Qin, Jiancong Xiao, Ruoyu Sun, Zhi-Quan Luo \n
  arXiv: 2408.16673 \n
  TL;DR: /This work develops a training method for supervised fine-tuning of LLMs, mitigating the overfitting and distribution collapse issues. /
~~~

~~~
  [https://arxiv.org/abs/2310.10505 ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models] \n
  *Ziniu Li*, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, Zhi-Quan Luo \n
  The 41st International Conference on Machine Learning (ICML), 2024  \n

  TL;DR: /This work develops an RL method called ReMax for RLHF in LLMs, which is simple (6 lines of code) and efficient (less memory and fast training) /
~~~


~~~
  [https://openreview.net/forum?id=lNEFatlsQb When is RL better than DPO in RLHF? A Representation and Optimization Perspective] \n
  *Ziniu Li\* *, Tian Xu\*, Yang Yu \n
  *Oral* Presentation, The 12th International Conference on Learning Representations (ICLR) (Tiny Paper Track), 2024 \n

  TL;DR: /This work analyzes the reward modeling quality in view of representations, and analyzed the optimization error sources /
~~~


~~~
  [https://openreview.net/forum?id=vO04AzsB49 Imitation Learning from Imperfection: Theoretical Justifications and Algorithms] \n
  *Ziniu Li\* *, Tian Xu\*, Zeyu Qin, Yang Yu, Zhi-Quan Luo \n
  *Spotlight* Presentation (acceptance rate < 5\%), In Neural Information Processing System (NeurIPS) 37, 2023 \n

  TL;DR: /This work validates that importance sampling is effective in data selection when leveraging multiple imperfect (out-of-distribution and low-quality) data sources/
~~~



== Service

==== Reviewer

NeurIPS ([https://neurips.cc/Conferences/2022/ProgramCommittee Top Reviewer]), ICML ([https://icml.cc/Conferences/2022/Reviewers Outstanding Reviewer]), ICLR ([https://iclr.cc/Conferences/2022/Reviewers Highlighted Reviewer]).

==== Teaching Assistant

- DDA6111: Discrete Optimization. 2022 Spring @ CUHKSZ

- DDA6060: Machine Learning. 2023 Spring @ CUHKSZ

- FTE4560: Basic Machine Learning. 2021 Spring @ CUHKSZ.

- CSC4120: Design and Analysis of Algorithms. 2022 Fall, 2021 Fall @ CUHKSZ

- MAT3007: Introduction to Optimization. 2020 Fall @ CUHKSZ

==== Lecturer

- Machine Learning (Summer Course for Senior High School Students) @ X ACADEMY 2022 TechX

== Award

- \[2024-01\] Runner-up of poster presentation award at the third doctoral and postdoctoral forum of Shenzhen Research Institute of Big Data. $5,000 CNY

- \[2023-12\] Guo-Tai-Jun-An Scholarship. $20,000 CNY

- \[2021-04\] Best oral presentation award at the first doctoral and postdoctoral forum of Shenzhen Research Institute of Big Data. $5,000 CNY
